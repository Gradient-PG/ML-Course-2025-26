{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# NLP fast tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd56f5405328c9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T22:16:15.739637Z",
     "start_time": "2025-12-08T22:16:13.965399Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8868a6cdfc1135b1",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T22:16:17.652494Z",
     "start_time": "2025-12-08T22:16:15.753970Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# production approach for large datasets\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "test = \"Beach-combing replaced wine tasting as his new obsession. \\\n",
    "That is an appealing treasure map that I can't read.\"\n",
    "\n",
    "print(\"Word: \", word_tokenize(test))\n",
    "print(\"Sentence: \", sent_tokenize(test))\n",
    "print(\"Tokenized: \", tokenizer.tokenize(test), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9019b073a32f550",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516daf2bc8bbdf61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T22:16:19.158275Z",
     "start_time": "2025-12-08T22:16:17.710066Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"plays\", 'v'))\n",
    "print(lemmatizer.lemmatize(\"played\", 'v'))\n",
    "print(lemmatizer.lemmatize(\"play\", 'v'))\n",
    "print(lemmatizer.lemmatize(\"playing\", 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faa2964c37d28a7",
   "metadata": {},
   "source": [
    "Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c0126941fde34f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T22:16:19.278192Z",
     "start_time": "2025-12-08T22:16:19.215728Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "text = \"I love ChatGPT\"\n",
    "tokenized_text = word_tokenize(text)\n",
    "tags = tokens_tag = pos_tag(tokenized_text)\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a1d11053275d3f",
   "metadata": {},
   "source": [
    "Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a6d5cceb39326",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T22:16:19.423317Z",
     "start_time": "2025-12-08T22:16:19.283105Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "text = \"Barack Obama was born in Hawaii in 1961.\"\n",
    "\n",
    "# Tokenize and POS tag the sentence\n",
    "tokens = word_tokenize(text)\n",
    "tags = pos_tag(tokens)\n",
    "\n",
    "# Apply Named Entity Recognition\n",
    "entities = ne_chunk(tags)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd0a5ebefd8b8ad",
   "metadata": {},
   "source": [
    "Stop World Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6d7f716bb57080",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T22:16:19.431316Z",
     "start_time": "2025-12-08T22:16:19.428903Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"This is an example sentence demonstrating the removal of stop words.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_words = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(tokens)\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff838aaa7ae044e2",
   "metadata": {},
   "source": [
    "Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3a9fba1b9935a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T22:16:22.033540Z",
     "start_time": "2025-12-08T22:16:19.488390Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import cli\n",
    "\n",
    "cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(f\"{'Word':<12} {'Dependency':<12} {'Head Word':<12}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for token in doc:\n",
    "    # token.text: The word itself\n",
    "    # token.dep_: The relationship (e.g., nsubj = nominal subject)\n",
    "    # token.head.text: The word this word is attached to\n",
    "    print(f\"{token.text:<12} {token.dep_:<12} {token.head.text:<12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715fbe915759194",
   "metadata": {},
   "source": [
    "Coreference Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c472b8f6e6607a33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T22:16:23.710818Z",
     "start_time": "2025-12-08T22:16:22.090327Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastcoref import FCoref\n",
    "\n",
    "model = FCoref(device='cpu')\n",
    "\n",
    "text = \"Barack Obama was born in Hawaii. He was the 44th president.\"\n",
    "\n",
    "preds = model.predict(texts=[text])\n",
    "\n",
    "clusters = preds[0].get_clusters()\n",
    "print(\"Coreference Clusters:\", clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8fa13d1d85385",
   "metadata": {},
   "source": [
    "Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857233e8f4f79b67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T22:16:26.408194Z",
     "start_time": "2025-12-08T22:16:23.764790Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2').to(\"cuda\")\n",
    "\n",
    "# 2. Define data\n",
    "sentences = [\n",
    "    \"The cat sits on the mat\",\n",
    "    \"A dog is laying on the rug\",  # Semantically similar to the first\n",
    "    \"I love eating pizza\",         # Semantically different\n",
    "]\n",
    "\n",
    "# 3. Generate embeddings\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "print(f\"Shape: {embeddings.shape}\")  # (3, 384) -> 3 sentences, 384 dimensions each\n",
    "print(f\"First 5 values of sentence 1: {embeddings[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55852cae2771e318",
   "metadata": {},
   "source": [
    "RNN example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a06c78055081e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T22:16:26.463319Z",
     "start_time": "2025-12-08T22:16:26.461066Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d898ba71bde1e",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849148e953f53c88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T22:16:26.511575Z",
     "start_time": "2025-12-08T22:16:26.508859Z"
    }
   },
   "outputs": [],
   "source": [
    "name_dataset = {\n",
    "    'Italian': [\n",
    "        'Rossi', 'Russo', 'Ferrari', 'Esposito', 'Bianchi',\n",
    "        'Romano', 'Colombo', 'Ricci', 'Marino', 'Greco',\n",
    "        'Bruno', 'Gallo', 'Conti', 'De Luca', 'Mancini'\n",
    "    ],\n",
    "    'Japanese': [\n",
    "        'Sato', 'Suzuki', 'Takahashi', 'Tanaka', 'Watanabe',\n",
    "        'Ito', 'Yamamoto', 'Nakamura', 'Kobayashi', 'Kato',\n",
    "        'Yoshida', 'Yamada', 'Sasaki', 'Yamaguchi', 'Matsumoto'\n",
    "    ],\n",
    "    'English': [\n",
    "        'Smith', 'Jones', 'Taylor', 'Brown', 'Williams',\n",
    "        'Wilson', 'Johnson', 'Davies', 'Robinson', 'Wright',\n",
    "        'Thompson', 'Evans', 'Walker', 'White', 'Roberts'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4949e8ac32d547e3",
   "metadata": {},
   "source": [
    "Sequence of letters (tokenize by character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62d3aace348bbc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T22:21:37.695158Z",
     "start_time": "2025-12-08T22:21:37.692770Z"
    }
   },
   "outputs": [],
   "source": [
    "test = \"Sato\"\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return list(sentence)\n",
    "\n",
    "test_tokenized = tokenize(test)\n",
    "print(test_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41dbb2b618a109a",
   "metadata": {},
   "source": [
    "Vector Embeddings (little overengineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3383fb5857bf44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T22:22:19.577864Z",
     "start_time": "2025-12-08T22:22:19.565966Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = model.encode(test_tokenized, convert_to_tensor=True)\n",
    "print(embeddings)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad8294c139348a2",
   "metadata": {},
   "source": [
    "Creating dataset and preprocess it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e775afd174c841",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T23:05:46.276306Z",
     "start_time": "2025-12-08T23:05:45.995170Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "categories = list(name_dataset.keys())\n",
    "def category_to_index(category):\n",
    "    return categories.index(category)\n",
    "\n",
    "def index_to_category(index):\n",
    "    return categories[index]\n",
    "\n",
    "data_pairs = []\n",
    "\n",
    "for key, list_value in name_dataset.items():\n",
    "    category_index = category_to_index(key)\n",
    "    for value in list_value:\n",
    "        data_pairs.append((tokenize(value), category_index))\n",
    "\n",
    "random.shuffle(data_pairs)\n",
    "\n",
    "X_raw = [model.encode(pair[0], convert_to_tensor=True) for pair in data_pairs] # The names\n",
    "Y_raw = [torch.tensor([pair[1]], dtype=torch.long) for pair in data_pairs]\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X_raw, Y_raw, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Total samples: {len(data_pairs)}\")\n",
    "print(f\"Training samples: {len(train_X)}\")\n",
    "print(f\"Test samples: {len(test_X)}\")\n",
    "print(f\"Sample X shape: {train_X[0].shape}\")\n",
    "print(f\"Sample X type: {train_X[0].dtype}\")\n",
    "print(f\"Sample Y shape: {train_Y[0].shape}\")\n",
    "print(f\"Sample Y type: {train_Y[0].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d289d5fd51964d",
   "metadata": {},
   "source": [
    "RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec54bbec45a05e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T23:05:50.434540Z",
     "start_time": "2025-12-08T23:05:50.431964Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNForEmbeddings(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNForEmbeddings, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x is now expected to be a FloatTensor of vectors.\n",
    "        # Expected Shape: [Sequence Length, Vector Dimension] (e.g., [4, 32])\n",
    "\n",
    "        # Add the batch dimension if it's missing (make it [1, 4, 32])\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        # Run the RNN\n",
    "        # It accepts the vectors directly\n",
    "        out, hidden = self.rnn(x)\n",
    "\n",
    "        # Pass the final hidden state to the classifier\n",
    "        final_output = self.fc(hidden.squeeze(0))\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a91918eb6cb72d",
   "metadata": {},
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65de7a8791888ac0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T23:05:52.114198Z",
     "start_time": "2025-12-08T23:05:52.107548Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden = 128\n",
    "n_letters = train_X[0].shape[1]\n",
    "n_epochs = 1000 # How many times we show it a name\n",
    "learning_rate = 0.005\n",
    "\n",
    "# Instantiate the model\n",
    "# input_size = 384 (number of letters)\n",
    "# hidden_size = 128 (size of the \"brain\")\n",
    "# output_size = 3 (Italian, Japanese, English)\n",
    "rnn = RNNForEmbeddings(input_size=n_letters, hidden_size=n_hidden, output_size=len(categories))\n",
    "\n",
    "rnn.to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb2e7e49448a6fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T23:06:58.481670Z",
     "start_time": "2025-12-08T23:06:55.980503Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Helper to track loss\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "print(f\"Training on {len(train_X)} samples for {n_epochs} iterations...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # 1. Pick a random training example\n",
    "    # We use random sampling (Stochastic Gradient Descent)\n",
    "    random_index = random.randint(0, len(train_X) - 1)\n",
    "    input_tensor, target_tensor = train_X[random_index], train_Y[random_index]\n",
    "    input_tensor = input_tensor.clone().to(device)\n",
    "    target_tensor = target_tensor.clone().to(device)\n",
    "\n",
    "    # 2. Zero the gradients (PyTorch accumulates them by default)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 3. Forward Pass: Feed the name into the RNN\n",
    "    output = rnn(input_tensor)\n",
    "\n",
    "    # 4. Calculate Loss: How wrong was it?\n",
    "    # output shape: [3] (scores for It, Jap, Eng)\n",
    "    # target shape: [1] (the correct index)\n",
    "    loss = criterion(output, target_tensor)\n",
    "\n",
    "    # 5. Backward Pass: Calculate gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # 6. Optimizer Step: Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # --- Logging ---\n",
    "    current_loss += loss.item()\n",
    "\n",
    "    # Print updates every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        avg_loss = current_loss / 100\n",
    "        print(f\"Epoch {epoch} | Loss: {avg_loss:.4f}\")\n",
    "        all_losses.append(avg_loss)\n",
    "        current_loss = 0\n",
    "\n",
    "print(f\"Training finished in {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237d2701679674be",
   "metadata": {},
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb078dd8154cf80f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T23:17:46.639791Z",
     "start_time": "2025-12-08T23:17:46.539849Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Set the RNN (not the embedding model) to eval mode\n",
    "rnn.eval()\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# 2. Iterate through test data\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_X)):\n",
    "        inputs = test_X[i].to(device)\n",
    "\n",
    "        label = test_Y[i]\n",
    "\n",
    "        outputs = rnn(inputs)\n",
    "\n",
    "        _, prediction = torch.max(outputs, 1)\n",
    "\n",
    "        y_true.append(label.item())\n",
    "        y_pred.append(prediction.item())\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# 4. Plot Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Using the categories names for the axis labels makes it easier to read\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=categories, yticklabels=categories)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959e01f6db4aaa0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T23:19:18.109114Z",
     "start_time": "2025-12-08T23:19:18.088582Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_name(name):\n",
    "    rnn.eval()\n",
    "    with torch.no_grad():\n",
    "        # 1. Preprocess the raw string exactly like we did for training data\n",
    "        tokens = tokenize(name)\n",
    "\n",
    "        # 2. Generate Embeddings\n",
    "        embedding = model.encode(tokens, convert_to_tensor=True).clone()\n",
    "\n",
    "        # 3. Move to Device\n",
    "        embedding = embedding.to(device)\n",
    "\n",
    "        # 4. Run Model\n",
    "        output = rnn(embedding)\n",
    "\n",
    "        # 5. Get Result\n",
    "        _, top_index = output.topk(1)\n",
    "        category = categories[top_index.item()]\n",
    "\n",
    "        return category\n",
    "\n",
    "# --- Try it out! ---\n",
    "print(\"\\n--- Live Tests ---\")\n",
    "print(f\"Test 'Mussolini': {predict_name('Mussolini')}\")\n",
    "print(f\"Test 'Nakamoto': {predict_name('Nakamoto')}\")\n",
    "print(f\"Test 'Shakespeare': {predict_name('Shakespeare')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a440e00a2078f2",
   "metadata": {},
   "source": [
    "# Exercise:\n",
    "Create a spam detector using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5cfad90d51b414",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T23:25:51.935407Z",
     "start_time": "2025-12-08T23:25:50.358422Z"
    }
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "zip_path = \"smsspamcollection.zip\"\n",
    "extract_path = \"sms_data\"\n",
    "\n",
    "if not os.path.exists(extract_path):\n",
    "    print(\"Downloading dataset...\")\n",
    "    urlretrieve(url, zip_path)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "# Load data using Pandas\n",
    "# The UCI dataset is tab-separated (TSV) with no header\n",
    "df = pd.read_csv(f\"{extract_path}/SMSSpamCollection\", sep='\\t', names=['label', 'text'])\n",
    "\n",
    "# Convert labels to numbers: spam=1, ham=0\n",
    "df['label'] = df['label'].map({'spam': 1, 'ham': 0})\n",
    "print(f\"Data Loaded: {len(df)} messages.\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdca6155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
